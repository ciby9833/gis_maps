#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
GIS åç«¯æœåŠ¡ - ä¸šåŠ¡é€»è¾‘æ¨¡å—ï¼ˆé«˜å¹¶å‘ä¼˜åŒ–ç‰ˆæœ¬ï¼‰
åŒ…å«æ•°æ®åº“è¿æ¥ã€Redisç¼“å­˜ã€ç¼©æ”¾ç­–ç•¥ã€åœ°ç†ç¼–ç ç­‰æ ¸å¿ƒä¸šåŠ¡é€»è¾‘
"""

import asyncio
import json
import time
import hashlib
import logging
import requests
import re
import urllib.parse
import os
from typing import Optional, List, Dict, Any, Union

# åŠ¨æ€å¯¼å…¥é…ç½®ï¼Œå¤„ç†å¯èƒ½ä¸å­˜åœ¨çš„é…ç½®é¡¹
try:
    from config import (
        DB_CONFIG, ASYNC_DB_CONFIG, GOOGLE_GEOCODING_CONFIG, SEARCH_CONFIG
    )
    
    # å°è¯•å¯¼å…¥æ–°çš„é«˜å¹¶å‘é…ç½®é¡¹ï¼Œå¦‚æœä¸å­˜åœ¨åˆ™ä½¿ç”¨é»˜è®¤å€¼
    try:
        from config import READ_REPLICA_CONFIGS, REDIS_CONFIG, CACHE_CONFIG, MONITORING_CONFIG
    except ImportError:
        # é»˜è®¤é…ç½®
        READ_REPLICA_CONFIGS = []
        REDIS_CONFIG = {
            'host': 'localhost',
            'port': 6379,
            'db': 0,
            'password': None,
            'socket_timeout': 5,
            'connection_pool_max_connections': 100,
            'decode_responses': True,
            'health_check_interval': 30
        }
        CACHE_CONFIG = {
            'memory_cache': {'max_size': 1000, 'ttl': 300},
            'redis_cache': {
                'buildings_ttl': 3600,
                'land_polygons_ttl': 7200,
                'roads_ttl': 1800,
                'pois_ttl': 1800,
                'max_geojson_size': 10 * 1024 * 1024,
            },
            'precompute_cache': {
                'enabled': True,
                'zoom_levels': [6, 8, 10, 12, 14, 16, 18],
                'update_interval': 3600,
            }
        }
        MONITORING_CONFIG = {
            'enabled': True,
            'slow_query_threshold': 1.0,
            'enable_profiling': True
        }
        
except ImportError:
    # å¦‚æœconfig.pyä¸å­˜åœ¨ï¼Œä½¿ç”¨é»˜è®¤é…ç½®
    DB_CONFIG = {
        'host': 'localhost',
        'port': 5432,
        'database': 'gisdb',
        'user': 'postgres',
        'password': 'xiaotao4vip'
    }
    
    ASYNC_DB_CONFIG = {
        'host': 'localhost',
        'port': 5432,
        'database': 'gisdb',
        'user': 'postgres',
        'password': 'xiaotao4vip',
        'min_size': 100,
        'max_size': 500,
        'max_queries': 100000,
        'max_inactive_connection_lifetime': 600,
        'command_timeout': 120,
        'server_settings': {
            'application_name': 'gis_map_service_hc',
            'timezone': 'UTC'
        }
    }
    
    READ_REPLICA_CONFIGS = []
    REDIS_CONFIG = {
        'host': 'localhost',
        'port': 6379,
        'db': 0,
        'password': None,
        'socket_timeout': 5,
        'connection_pool_max_connections': 100,
        'decode_responses': True,
        'health_check_interval': 30
    }
    
    CACHE_CONFIG = {
        'memory_cache': {'max_size': 1000, 'ttl': 300},
        'redis_cache': {
            'buildings_ttl': 3600,
            'land_polygons_ttl': 7200,
            'roads_ttl': 1800,
            'pois_ttl': 1800,
            'max_geojson_size': 10 * 1024 * 1024,
        }
    }
    
    GOOGLE_GEOCODING_CONFIG = {
        'api_key': os.getenv('GOOGLE_API_KEY', 'YOUR_GOOGLE_API_KEY_HERE'),
        'base_url': 'https://maps.googleapis.com/maps/api/geocode/json',
        'language': os.getenv('GOOGLE_LANGUAGE', 'id,en'),
        'region': os.getenv('GOOGLE_REGION', 'id'),
        'timeout': int(os.getenv('GOOGLE_TIMEOUT', '5')),
        'max_retries': int(os.getenv('GOOGLE_MAX_RETRIES', '3')),
        'backoff_factor': 0.3
    }
    
    SEARCH_CONFIG = {
        'max_results': 50,
        'radius_meters': 100,
        'max_distance': 20000
    }
    
    MONITORING_CONFIG = {
        'enabled': True,
        'slow_query_threshold': 1.0,
        'enable_profiling': True
    }

# é…ç½®æ—¥å¿—
logger = logging.getLogger("gis_backend")

# ç»Ÿä¸€æ•°æ®åº“æŸ¥è¯¢è¶…æ—¶é…ç½®
DB_QUERY_TIMEOUT = 120.0

# å…¨å±€è¿æ¥æ± å’Œç¼“å­˜
db_pool = None
read_pools = []
redis_client = None
memory_cache = {}
cache_stats = {'hits': 0, 'misses': 0, 'redis_hits': 0, 'redis_misses': 0}

# ==============================================================================
# GeoJSONæ•°æ®æ¸…ç†å·¥å…·
# ==============================================================================

def clean_geojson_features(geojson):
    """ç§»é™¤ geometry ä¸º null æˆ– 'null' çš„ feature"""
    if not geojson or "features" not in geojson:
        return geojson
    
    cleaned_features = []
    for f in geojson["features"]:
        # æ£€æŸ¥featureæœ¬èº«æ˜¯å¦æœ‰æ•ˆ
        if not f or not isinstance(f, dict):
            continue
            
        # æ£€æŸ¥geometryæ˜¯å¦æœ‰æ•ˆ
        geometry = f.get("geometry")
        if (geometry is None or 
            geometry == "null" or 
            geometry == "" or
            (isinstance(geometry, dict) and not geometry) or
            (isinstance(geometry, str) and geometry.strip() == "")):
            continue
            
        # å¦‚æœgeometryæ˜¯å­—ç¬¦ä¸²ä¸”ä¸ºJSONï¼Œå°è¯•è§£æ
        if isinstance(geometry, str):
            try:
                import json
                geometry = json.loads(geometry)
                f["geometry"] = geometry
            except:
                continue
                
        # æ£€æŸ¥è§£æåçš„geometryæ˜¯å¦æœ‰æ•ˆ
        if (not isinstance(geometry, dict) or
            not geometry.get("type") or
            not geometry.get("coordinates")):
            continue
            
        cleaned_features.append(f)
    
    geojson["features"] = cleaned_features
    return geojson

# ==============================================================================
# é«˜å¹¶å‘æ•°æ®åº“è¿æ¥ç®¡ç†
# ==============================================================================

async def init_db_pool():
    """åˆå§‹åŒ–æ•°æ®åº“è¿æ¥æ±  - é«˜å¹¶å‘ä¼˜åŒ–"""
    global db_pool, read_pools
    try:
        # åŠ¨æ€å¯¼å…¥asyncpg
        try:
            import asyncpg
        except ImportError:
            logger.error("asyncpgæœªå®‰è£…ï¼Œè¯·è¿è¡Œ: pip install asyncpg")
            raise
        
        # åˆ›å»ºä¸»æ•°æ®åº“è¿æ¥æ± ï¼ˆå†™æ“ä½œï¼‰
        db_pool = await asyncpg.create_pool(**ASYNC_DB_CONFIG)
        logger.info(f"ä¸»æ•°æ®åº“è¿æ¥æ± å·²åˆ›å»º: min_size={ASYNC_DB_CONFIG['min_size']}, max_size={ASYNC_DB_CONFIG['max_size']}")
        
        # åˆ›å»ºè¯»å‰¯æœ¬è¿æ¥æ± 
        for i, read_config in enumerate(READ_REPLICA_CONFIGS):
            read_pool = await asyncpg.create_pool(**read_config)
            read_pools.append(read_pool)
            logger.info(f"è¯»å‰¯æœ¬{i+1}è¿æ¥æ± å·²åˆ›å»º: min_size={read_config['min_size']}, max_size={read_config['max_size']}")
        
        # åˆå§‹åŒ–Redisç¼“å­˜
        await init_redis_cache()
        
        return db_pool
        
    except Exception as e:
        logger.error(f"æ•°æ®åº“è¿æ¥æ± åˆ›å»ºå¤±è´¥: {e}")
        raise

async def init_redis_cache():
    """åˆå§‹åŒ–Redisç¼“å­˜è¿æ¥"""
    global redis_client
    try:
        try:
            import redis.asyncio as redis
        except ImportError:
            logger.warning("redisæœªå®‰è£…ï¼Œè¯·è¿è¡Œ: pip install redis")
            redis_client = None
            return
        
        redis_client = redis.Redis(
            host=REDIS_CONFIG['host'],
            port=REDIS_CONFIG['port'],
            db=REDIS_CONFIG['db'],
            password=REDIS_CONFIG['password'],
            socket_timeout=REDIS_CONFIG['socket_timeout'],
            decode_responses=REDIS_CONFIG['decode_responses'],
            health_check_interval=REDIS_CONFIG['health_check_interval']
        )
        
        # æµ‹è¯•è¿æ¥
        await redis_client.ping()
        logger.info("Redisç¼“å­˜è¿æ¥å·²å»ºç«‹")
        
    except Exception as e:
        logger.warning(f"Redisè¿æ¥å¤±è´¥: {e}")
        redis_client = None

async def get_db_connection(read_only=False):
    """è·å–æ•°æ®åº“è¿æ¥ - æ”¯æŒè¯»å†™åˆ†ç¦»"""
    global db_pool, read_pools
    
    # ğŸ”¥ å…³é”®ä¿®å¤ï¼šä¸è¦åœ¨è¿™é‡Œåˆ›å»ºè¿æ¥æ± ï¼
    # è¿æ¥æ± åº”è¯¥åœ¨åº”ç”¨å¯åŠ¨æ—¶åˆ›å»ºä¸€æ¬¡ï¼Œè€Œä¸æ˜¯æ¯æ¬¡è¯·æ±‚éƒ½åˆ›å»º
    if db_pool is None:
        raise Exception("æ•°æ®åº“è¿æ¥æ± æœªåˆå§‹åŒ–ï¼Œè¯·ç¡®ä¿åœ¨åº”ç”¨å¯åŠ¨æ—¶è°ƒç”¨ init_db_pool()")
    
    # è¯»æ“ä½œä½¿ç”¨è¯»å‰¯æœ¬
    if read_only and read_pools:
        # ç®€å•è½®è¯¢è´Ÿè½½å‡è¡¡
        pool_index = int(time.time()) % len(read_pools)
        return read_pools[pool_index]
    
    # å†™æ“ä½œä½¿ç”¨ä¸»åº“
    return db_pool

def get_db_connection_sync():
    """è·å–æ•°æ®åº“è¿æ¥ (åŒæ­¥ç‰ˆæœ¬)"""
    try:
        import psycopg2
        conn = psycopg2.connect(**DB_CONFIG)
        return conn
    except Exception as e:
        logger.error(f"åŒæ­¥æ•°æ®åº“è¿æ¥å¤±è´¥: {e}")
        return None

async def close_db_pool():
    """å…³é—­æ•°æ®åº“è¿æ¥æ± """
    global db_pool, read_pools, redis_client
    
    if db_pool:
        await db_pool.close()
        logger.info("ä¸»æ•°æ®åº“è¿æ¥æ± å·²å…³é—­")
    
    for i, read_pool in enumerate(read_pools):
        if read_pool:
            await read_pool.close()
            logger.info(f"è¯»å‰¯æœ¬{i+1}è¿æ¥æ± å·²å…³é—­")
    
    if redis_client:
        await redis_client.close()
        logger.info("Redisè¿æ¥å·²å…³é—­")

# ==============================================================================
# å¤šå±‚ç¼“å­˜ç®¡ç† - é«˜å¹¶å‘ä¼˜åŒ–
# ==============================================================================

async def get_cache_value(key: str, cache_type: str = 'buildings') -> Optional[Dict]:
    """è·å–ç¼“å­˜å€¼ - æ”¯æŒå†…å­˜+Rediså¤šå±‚ç¼“å­˜"""
    global cache_stats
    
    # 1. æ£€æŸ¥å†…å­˜ç¼“å­˜
    if key in memory_cache:
        timestamp, value = memory_cache[key]
        if time.time() - timestamp < CACHE_CONFIG['memory_cache']['ttl']:
            cache_stats['hits'] += 1
            return value
        else:
            # è¿‡æœŸåˆ é™¤
            del memory_cache[key]
    
    # 2. æ£€æŸ¥Redisç¼“å­˜
    if redis_client:
        try:
            redis_value = await redis_client.get(key)
            if redis_value:
                cache_stats['redis_hits'] += 1
                value = json.loads(redis_value)
                
                # å›å†™åˆ°å†…å­˜ç¼“å­˜
                if len(memory_cache) < CACHE_CONFIG['memory_cache']['max_size']:
                    memory_cache[key] = (time.time(), value)
                
                return value
            else:
                cache_stats['redis_misses'] += 1
        except Exception as e:
            logger.warning(f"Redisè·å–å¤±è´¥: {e}")
    
    cache_stats['misses'] += 1
    return None

async def set_cache_value(key: str, value: Dict, cache_type: str = 'buildings'):
    """è®¾ç½®ç¼“å­˜å€¼ - æ”¯æŒå†…å­˜+Rediså¤šå±‚ç¼“å­˜"""
    try:
        # 1. è®¾ç½®å†…å­˜ç¼“å­˜
        if len(memory_cache) < CACHE_CONFIG['memory_cache']['max_size']:
            memory_cache[key] = (time.time(), value)
        
        # 2. è®¾ç½®Redisç¼“å­˜
        if redis_client:
            ttl = CACHE_CONFIG['redis_cache'].get(f'{cache_type}_ttl', 3600)
            
            # æ£€æŸ¥å¤§å°é™åˆ¶
            json_str = json.dumps(value)
            if len(json_str.encode()) <= CACHE_CONFIG['redis_cache']['max_geojson_size']:
                await redis_client.setex(key, ttl, json_str)
            else:
                logger.warning(f"ç¼“å­˜å€¼è¿‡å¤§ï¼Œè·³è¿‡Redisç¼“å­˜: {len(json_str.encode())} bytes")
                
    except Exception as e:
        logger.warning(f"è®¾ç½®ç¼“å­˜å¤±è´¥: {e}")

def get_cache_key(endpoint: str, **params) -> str:
    """ç”Ÿæˆç¼“å­˜é”® - ä¼˜åŒ–ç‰ˆæœ¬"""
    # ç§»é™¤ç©ºå€¼å‚æ•°
    filtered_params = {k: v for k, v in params.items() if v is not None}
    
    # å¯¹bboxè¿›è¡Œæ ‡å‡†åŒ–ï¼ˆå‡å°‘ç¼“å­˜ç¢ç‰‡ï¼‰
    if 'bbox' in filtered_params:
        bbox = filtered_params['bbox']
        if isinstance(bbox, str):
            coords = [float(x) for x in bbox.split(',')]
            # æ ‡å‡†åŒ–åˆ°å°æ•°ç‚¹å4ä½
            normalized_bbox = ','.join([f"{x:.4f}" for x in coords])
            filtered_params['bbox'] = normalized_bbox
    
    param_str = json.dumps(filtered_params, sort_keys=True)
    cache_hash = hashlib.md5(param_str.encode()).hexdigest()
    return f"{endpoint}:{cache_hash}"

def is_cache_valid(cache_key: str) -> bool:
    """æ£€æŸ¥ç¼“å­˜æ˜¯å¦æœ‰æ•ˆ - åºŸå¼ƒï¼Œä½¿ç”¨get_cache_value"""
    return cache_key in memory_cache

def clear_cache():
    """æ¸…ç†æ‰€æœ‰ç¼“å­˜"""
    global memory_cache, cache_stats
    
    # æ¸…ç†å†…å­˜ç¼“å­˜
    memory_cache.clear()
    
    # æ¸…ç†Redisç¼“å­˜ï¼ˆå¯é€‰ï¼‰
    if redis_client:
        try:
            asyncio.create_task(redis_client.flushdb())
        except Exception as e:
            logger.warning(f"æ¸…ç†Redisç¼“å­˜å¤±è´¥: {e}")
    
    # é‡ç½®ç»Ÿè®¡
    cache_stats = {'hits': 0, 'misses': 0, 'redis_hits': 0, 'redis_misses': 0}
    logger.info("ç¼“å­˜å·²æ¸…ç†")

def get_cache_stats() -> Dict:
    """è·å–ç¼“å­˜ç»Ÿè®¡ä¿¡æ¯"""
    total_requests = cache_stats['hits'] + cache_stats['misses']
    memory_hit_rate = (cache_stats['hits'] / total_requests * 100) if total_requests > 0 else 0
    
    redis_requests = cache_stats['redis_hits'] + cache_stats['redis_misses']
    redis_hit_rate = (cache_stats['redis_hits'] / redis_requests * 100) if redis_requests > 0 else 0
    
    return {
        'memory_cache': {
            'size': len(memory_cache),
            'max_size': CACHE_CONFIG['memory_cache']['max_size'],
            'hit_rate': f"{memory_hit_rate:.2f}%",
            'hits': cache_stats['hits'],
            'misses': cache_stats['misses']
        },
        'redis_cache': {
            'available': redis_client is not None,
            'hit_rate': f"{redis_hit_rate:.2f}%",
            'hits': cache_stats['redis_hits'],
            'misses': cache_stats['redis_misses']
        },
        'total_requests': total_requests
    }

# ==============================================================================
# ç¼©æ”¾çº§åˆ«ç­–ç•¥å‡½æ•° - é«˜å¹¶å‘ä¼˜åŒ–
# ==============================================================================

def get_zoom_strategy(zoom):
    """æ ¹æ®ç¼©æ”¾çº§åˆ«è¿”å›åŠ è½½ç­–ç•¥ - ä¼˜åŒ–ç‰ˆæœ¬"""
    if zoom is None or zoom < 6:
        return {
            'load_data': False,
            'reason': 'zoom_too_low',
            'max_features': 0,
            'simplify_tolerance': 0,
            'attributes': [],
            'cache_ttl': 7200  # 2å°æ—¶
        }
    elif zoom < 10:  # Z6-Z9: è¶…ç®€åŒ–æ•°æ®
        return {
            'load_data': True,
            'reason': 'overview_data',
            'max_features': 5000,  # å¢åŠ ç‰¹å¾æ•°é‡
            'simplify_tolerance': 100,  # 100ç±³ç®€åŒ–
            'attributes': ['id', 'osm_id', 'name', 'fclass', 'type', 'geometry_type', 'source_table'],
            'cache_ttl': 3600  # 1å°æ—¶
        }
    elif zoom < 12:  # Z10-Z11: ç®€åŒ–æ•°æ®
        return {
            'load_data': True,
            'reason': 'simplified_data',
            'max_features': 15000,  # å¢åŠ ç‰¹å¾æ•°é‡
            'simplify_tolerance': 20,  # 20ç±³ç®€åŒ–
            'attributes': ['id', 'osm_id', 'name', 'fclass', 'type', 'geometry_type', 'source_table'],
            'cache_ttl': 1800  # 30åˆ†é’Ÿ
        }
    elif zoom < 15:  # Z12-Z14: è¯¦ç»†æ•°æ®
        return {
            'load_data': True,
            'reason': 'detailed_data',
            'max_features': 30000,  # å¢åŠ ç‰¹å¾æ•°é‡
            'simplify_tolerance': 5,   # 5ç±³ç®€åŒ–
            'attributes': ['id', 'osm_id', 'name', 'fclass', 'type', 'code', 'geometry_type', 'source_table'],
            'cache_ttl': 900   # 15åˆ†é’Ÿ
        }
    else:  # Z15+: å…¨ç²¾åº¦æ•°æ®
        return {
            'load_data': True,
            'reason': 'full_precision',
            'max_features': 50000,  # å¢åŠ ç‰¹å¾æ•°é‡
            'simplify_tolerance': 1,   # 1ç±³ç®€åŒ–
            'attributes': ['id', 'osm_id', 'name', 'fclass', 'type', 'code', 'geometry_type', 'source_table'],
            'cache_ttl': 300   # 5åˆ†é’Ÿ
        }

def get_land_polygon_zoom_strategy(zoom):
    """æ ¹æ®ç¼©æ”¾çº§åˆ«è¿”å›é™†åœ°å¤šè¾¹å½¢åŠ è½½ç­–ç•¥ - ä¼˜åŒ–ç‰ˆæœ¬"""
    if zoom is None or zoom < 4:
        return {
            'load_data': False,
            'reason': 'zoom_too_low',
            'max_features': 0,
            'simplify_tolerance': 0,
            'cache_ttl': 7200
        }
    elif zoom < 8:  # Z4-Z7: æç®€åŒ–æ•°æ®
        return {
            'load_data': True,
            'reason': 'extremely_simplified',
            'max_features': 2000,  # å¢åŠ ç‰¹å¾æ•°é‡
            'simplify_tolerance': 2000,  # 2å…¬é‡Œç®€åŒ–
            'cache_ttl': 7200
        }
    elif zoom < 10:  # Z8-Z9: é«˜åº¦ç®€åŒ–æ•°æ®
        return {
            'load_data': True,
            'reason': 'highly_simplified',
            'max_features': 5000,  # å¢åŠ ç‰¹å¾æ•°é‡
            'simplify_tolerance': 1000,  # 1å…¬é‡Œç®€åŒ–
            'cache_ttl': 3600
        }
    elif zoom < 12:  # Z10-Z11: ä¸­åº¦ç®€åŒ–æ•°æ®
        return {
            'load_data': True,
            'reason': 'moderately_simplified',
            'max_features': 8000,  # å¢åŠ ç‰¹å¾æ•°é‡
            'simplify_tolerance': 200,  # 200ç±³ç®€åŒ–
            'cache_ttl': 1800
        }
    elif zoom < 14:  # Z12-Z13: è½»åº¦ç®€åŒ–æ•°æ®
        return {
            'load_data': True,
            'reason': 'lightly_simplified',
            'max_features': 6000,  # å¹³è¡¡æ€§èƒ½
            'simplify_tolerance': 50,   # 50ç±³ç®€åŒ–
            'cache_ttl': 900
        }
    else:  # Z14+: é«˜ç²¾åº¦æ•°æ®
        return {
            'load_data': True,
            'reason': 'high_precision',
            'max_features': 3000,  # æ§åˆ¶æ•°é‡
            'simplify_tolerance': 10,   # 10ç±³ç®€åŒ–
            'cache_ttl': 300
        }

def get_roads_zoom_strategy(zoom):
    """æ ¹æ®ç¼©æ”¾çº§åˆ«è¿”å›é“è·¯ç½‘ç»œåŠ è½½ç­–ç•¥ - ä¼˜åŒ–ç‰ˆæœ¬"""
    if zoom is None or zoom < 9:  # é™ä½æœ€ä½ç¼©æ”¾çº§åˆ«
        return {
            'load_data': False,
            'reason': 'zoom_too_low',
            'max_features': 0,
            'simplify_tolerance': 0,
            'cache_ttl': 7200
        }
    elif zoom < 11:  # Z9-Z10: ä¸»è¦é“è·¯
        return {
            'load_data': True,
            'reason': 'major_roads_only',
            'max_features': 5000,  # å¢åŠ ç‰¹å¾æ•°é‡
            'simplify_tolerance': 100,  # 100ç±³ç®€åŒ–
            'road_filter': "fclass IN ('primary', 'trunk', 'motorway', 'motorway_link', 'trunk_link', 'primary_link')",
            'cache_ttl': 3600
        }
    elif zoom < 13:  # Z11-Z12: ä¸»è¦é“è·¯å’Œæ¬¡è¦é“è·¯
        return {
            'load_data': True,
            'reason': 'major_and_secondary_roads',
            'max_features': 10000,  # å¢åŠ ç‰¹å¾æ•°é‡
            'simplify_tolerance': 50,  # 50ç±³ç®€åŒ–
            'road_filter': "fclass IN ('primary', 'trunk', 'motorway', 'motorway_link', 'trunk_link', 'primary_link', 'secondary', 'secondary_link')",
            'cache_ttl': 1800
        }
    elif zoom < 15:  # Z13-Z14: åŒ…å«ä¸‰çº§é“è·¯
        return {
            'load_data': True,
            'reason': 'all_major_roads',
            'max_features': 20000,  # å¢åŠ ç‰¹å¾æ•°é‡
            'simplify_tolerance': 20,  # 20ç±³ç®€åŒ–
            'road_filter': "fclass NOT IN ('track', 'path', 'footway', 'cycleway', 'bridleway', 'steps')",
            'cache_ttl': 900
        }
    else:  # Z15+: æ‰€æœ‰é“è·¯
        return {
            'load_data': True,
            'reason': 'all_roads',
            'max_features': 30000,  # å¢åŠ ç‰¹å¾æ•°é‡
            'simplify_tolerance': 10,  # 10ç±³ç®€åŒ–
            'road_filter': "1=1",  # æ‰€æœ‰é“è·¯
            'cache_ttl': 300
        }

# ==============================================================================
# åœ°ç†ç¼–ç å·¥å…·å‡½æ•° - é«˜å¹¶å‘ä¼˜åŒ–
# ==============================================================================

def is_coordinate_string(query):
    """æ£€æŸ¥è¾“å…¥æ˜¯å¦ä¸ºåæ ‡æ ¼å¼"""
    coord_patterns = [
        r'^-?\d+\.?\d*,\s*-?\d+\.?\d*$',
        r'^\(-?\d+\.?\d*,\s*-?\d+\.?\d*\)$',
        r'^-?\d+\.?\d*\s*,\s*-?\d+\.?\d*$',
    ]
    
    for pattern in coord_patterns:
        if re.match(pattern, query.strip()):
            return True
    return False

def parse_coordinates(query):
    """è§£æåæ ‡å­—ç¬¦ä¸²"""
    query = query.strip().replace('(', '').replace(')', '')
    parts = query.split(',')
    if len(parts) == 2:
        try:
            lng = float(parts[0].strip())
            lat = float(parts[1].strip())
            
            # éªŒè¯åæ ‡èŒƒå›´
            if lat < -90 or lat > 90 or lng < -180 or lng > 180:
                return None, None
            
            return lat, lng
        except ValueError:
            return None, None
    return None, None

# ==============================================================================
# Googleåœ°ç†ç¼–ç æœåŠ¡ - é«˜å¹¶å‘ä¼˜åŒ–
# ==============================================================================

async def google_geocode(address: str) -> Optional[Dict]:
    """ä½¿ç”¨Google Geocoding APIè·å–åœ°å€çš„åæ ‡ - é«˜å¹¶å‘ä¼˜åŒ–ç‰ˆæœ¬"""
    start_time = time.time()
    
    try:
        logger.info(f"ğŸŒ å¼€å§‹Googleåœ°ç†ç¼–ç è¯·æ±‚ - åœ°å€: {address}")
        
        # æ£€æŸ¥ç¼“å­˜
        cache_key = f"geocode:{hashlib.md5(address.encode()).hexdigest()}"
        cached_result = await get_cache_value(cache_key, 'geocode')
        if cached_result:
            elapsed_time = (time.time() - start_time) * 1000
            logger.info(f"âœ… Googleåœ°ç†ç¼–ç ç¼“å­˜å‘½ä¸­ - åœ°å€: {address}, è€—æ—¶: {elapsed_time:.2f}ms")
            logger.debug(f"ğŸ“ ç¼“å­˜ç»“æœ: lat={cached_result.get('lat')}, lng={cached_result.get('lng')}, address={cached_result.get('formatted_address')}")
            return cached_result
        
        # æ£€æŸ¥APIå¯†é’¥
        if not GOOGLE_GEOCODING_CONFIG['api_key'] or GOOGLE_GEOCODING_CONFIG['api_key'] == 'YOUR_GOOGLE_API_KEY_HERE':
            raise Exception("Google APIå¯†é’¥æœªé…ç½®")
        
        try:
            import aiohttp
        except ImportError:
            logger.error("aiohttpæœªå®‰è£…ï¼Œè¯·è¿è¡Œ: pip install aiohttp")
            return None
        
        params = {
            'address': address,
            'key': GOOGLE_GEOCODING_CONFIG['api_key'],
            'language': GOOGLE_GEOCODING_CONFIG['language'],
            'region': GOOGLE_GEOCODING_CONFIG['region']
        }
        
        # è®°å½•è¯·æ±‚å‚æ•°ï¼ˆéšè—APIå¯†é’¥ï¼‰
        safe_params = params.copy()
        safe_params['key'] = f"{params['key'][:10]}...{params['key'][-5:]}" if len(params['key']) > 15 else "***"
        logger.info(f"ğŸ” Googleåœ°ç†ç¼–ç è¯·æ±‚å‚æ•°: {safe_params}")
        
        # æ”¯æŒé‡è¯•æœºåˆ¶
        max_retries = GOOGLE_GEOCODING_CONFIG.get('max_retries', 3)
        backoff_factor = GOOGLE_GEOCODING_CONFIG.get('backoff_factor', 0.3)
        
        for attempt in range(max_retries):
            try:
                request_start = time.time()
                async with aiohttp.ClientSession(
                    timeout=aiohttp.ClientTimeout(total=GOOGLE_GEOCODING_CONFIG['timeout'])
                ) as session:
                    async with session.get(
                        GOOGLE_GEOCODING_CONFIG['base_url'],
                        params=params
                    ) as response:
                        request_time = (time.time() - request_start) * 1000
                        
                        if response.status == 200:
                            data = await response.json()
                            
                            # è®°å½•APIå“åº”çŠ¶æ€
                            logger.info(f"ğŸ“¡ Google APIå“åº”: status={data.get('status')}, è¯·æ±‚è€—æ—¶: {request_time:.2f}ms")
                            
                            if data.get('status') == 'OK':
                                results = data.get('results', [])
                                if results:
                                    location = results[0]['geometry']['location']
                                    result = {
                                        'lat': location['lat'],
                                        'lng': location['lng'],
                                        'formatted_address': results[0].get('formatted_address', address),
                                        'google_result': results[0]
                                    }
                                    
                                    # è®°å½•æˆåŠŸç»“æœ
                                    total_time = (time.time() - start_time) * 1000
                                    logger.info(f"âœ… Googleåœ°ç†ç¼–ç æˆåŠŸ - åœ°å€: {address}")
                                    logger.info(f"ğŸ“ ç»“æœ: lat={result['lat']}, lng={result['lng']}, formatted_address={result['formatted_address']}")
                                    logger.info(f"â±ï¸  æ€»è€—æ—¶: {total_time:.2f}ms (APIè¯·æ±‚: {request_time:.2f}ms)")
                                    
                                    # ç¼“å­˜ç»“æœ
                                    await set_cache_value(cache_key, result, 'geocode')
                                    return result
                                else:
                                    raise Exception("æœªæ‰¾åˆ°è¯¥åœ°å€çš„åæ ‡ä¿¡æ¯")
                            else:
                                error_msg = f"Google Geocoding APIé”™è¯¯: {data.get('status')}"
                                if data.get('error_message'):
                                    error_msg += f" - {data.get('error_message')}"
                                raise Exception(error_msg)
                        else:
                            raise Exception(f"HTTPé”™è¯¯: {response.status}")
                            
            except Exception as e:
                if attempt == max_retries - 1:
                    raise e
                retry_delay = backoff_factor * (2 ** attempt)
                logger.warning(f"âš ï¸  Googleåœ°ç†ç¼–ç é‡è¯• {attempt + 1}/{max_retries} - é”™è¯¯: {e}, {retry_delay:.2f}ç§’åé‡è¯•")
                await asyncio.sleep(retry_delay)
        
        return None
        
    except Exception as e:
        total_time = (time.time() - start_time) * 1000
        logger.error(f"âŒ Googleåœ°ç†ç¼–ç å¤±è´¥ - åœ°å€: {address}, é”™è¯¯: {e}, è€—æ—¶: {total_time:.2f}ms")
        return None

# ä¿ç•™åŸæœ‰çš„åŒæ­¥Googleåœ°ç†ç¼–ç å‡½æ•°ä»¥å…¼å®¹æ€§
def google_geocode_sync(address: str) -> Optional[Dict]:
    """åŒæ­¥ç‰ˆæœ¬çš„Googleåœ°ç†ç¼–ç """
    start_time = time.time()
    
    try:
        logger.info(f"ğŸŒ å¼€å§‹Googleåœ°ç†ç¼–ç è¯·æ±‚ï¼ˆåŒæ­¥ï¼‰ - åœ°å€: {address}")
        
        # ä½¿ç”¨requestsçš„åŒæ­¥ç‰ˆæœ¬
        cache_key = f"geocode:{hashlib.md5(address.encode()).hexdigest()}"
        
        # æ£€æŸ¥å†…å­˜ç¼“å­˜
        if cache_key in memory_cache:
            timestamp, result = memory_cache[cache_key]
            if time.time() - timestamp < 3600:  # 1å°æ—¶ç¼“å­˜
                elapsed_time = (time.time() - start_time) * 1000
                logger.info(f"âœ… Googleåœ°ç†ç¼–ç ç¼“å­˜å‘½ä¸­ï¼ˆåŒæ­¥ï¼‰ - åœ°å€: {address}, è€—æ—¶: {elapsed_time:.2f}ms")
                logger.debug(f"ğŸ“ ç¼“å­˜ç»“æœ: lat={result.get('lat')}, lng={result.get('lng')}, address={result.get('formatted_address')}")
                return result
        
        params = {
            'address': address,
            'key': GOOGLE_GEOCODING_CONFIG['api_key'],
            'language': GOOGLE_GEOCODING_CONFIG['language'],
            'region': GOOGLE_GEOCODING_CONFIG['region']
        }
        
        # è®°å½•è¯·æ±‚å‚æ•°ï¼ˆéšè—APIå¯†é’¥ï¼‰
        safe_params = params.copy()
        safe_params['key'] = f"{params['key'][:10]}...{params['key'][-5:]}" if len(params['key']) > 15 else "***"
        logger.info(f"ğŸ” Googleåœ°ç†ç¼–ç è¯·æ±‚å‚æ•°ï¼ˆåŒæ­¥ï¼‰: {safe_params}")
        
        request_start = time.time()
        response = requests.get(
            GOOGLE_GEOCODING_CONFIG['base_url'],
            params=params,
            timeout=GOOGLE_GEOCODING_CONFIG['timeout']
        )
        request_time = (time.time() - request_start) * 1000
        
        if response.status_code == 200:
            data = response.json()
            
            # è®°å½•APIå“åº”çŠ¶æ€
            logger.info(f"ğŸ“¡ Google APIå“åº”ï¼ˆåŒæ­¥ï¼‰: status={data.get('status')}, è¯·æ±‚è€—æ—¶: {request_time:.2f}ms")
            
            if data.get('status') == 'OK':
                results = data.get('results', [])
                if results:
                    location = results[0]['geometry']['location']
                    result = {
                        'lat': location['lat'],
                        'lng': location['lng'],
                        'formatted_address': results[0].get('formatted_address', address),
                        'google_result': results[0]
                    }
                    
                    # è®°å½•æˆåŠŸç»“æœ
                    total_time = (time.time() - start_time) * 1000
                    logger.info(f"âœ… Googleåœ°ç†ç¼–ç æˆåŠŸï¼ˆåŒæ­¥ï¼‰ - åœ°å€: {address}")
                    logger.info(f"ğŸ“ ç»“æœ: lat={result['lat']}, lng={result['lng']}, formatted_address={result['formatted_address']}")
                    logger.info(f"â±ï¸  æ€»è€—æ—¶: {total_time:.2f}ms (APIè¯·æ±‚: {request_time:.2f}ms)")
                    
                    # ç¼“å­˜ç»“æœ
                    memory_cache[cache_key] = (time.time(), result)
                    return result
                else:
                    logger.warning(f"âš ï¸  Googleåœ°ç†ç¼–ç æ— ç»“æœï¼ˆåŒæ­¥ï¼‰ - åœ°å€: {address}")
            else:
                error_msg = f"Google Geocoding APIé”™è¯¯: {data.get('status')}"
                if data.get('error_message'):
                    error_msg += f" - {data.get('error_message')}"
                logger.error(f"âŒ {error_msg}")
        else:
            logger.error(f"âŒ HTTPé”™è¯¯ï¼ˆåŒæ­¥ï¼‰: {response.status_code}")
                    
        return None
        
    except Exception as e:
        total_time = (time.time() - start_time) * 1000
        logger.error(f"âŒ åŒæ­¥Googleåœ°ç†ç¼–ç å¤±è´¥ - åœ°å€: {address}, é”™è¯¯: {e}, è€—æ—¶: {total_time:.2f}ms")
        return None

# ä¿ç•™åŸæœ‰çš„åå‘åœ°ç†ç¼–ç å‡½æ•°
def google_reverse_geocode(lat, lng):
    """ä½¿ç”¨Google Geocoding APIè¿›è¡Œåå‘åœ°ç†ç¼–ç è·å–åœ°å€ä¿¡æ¯"""
    start_time = time.time()
    
    try:
        logger.info(f"ğŸ” å¼€å§‹Googleåå‘åœ°ç†ç¼–ç è¯·æ±‚ - åæ ‡: ({lat}, {lng})")
        
        if not GOOGLE_GEOCODING_CONFIG['api_key'] or GOOGLE_GEOCODING_CONFIG['api_key'] == 'YOUR_GOOGLE_API_KEY_HERE':
            logger.error("âŒ Google APIå¯†é’¥æœªé…ç½®")
            return None
        
        # æ£€æŸ¥ç¼“å­˜
        cache_key = f"reverse_geocode:{lat:.6f}:{lng:.6f}"
        if cache_key in memory_cache:
            timestamp, result = memory_cache[cache_key]
            if time.time() - timestamp < 3600:  # 1å°æ—¶ç¼“å­˜
                elapsed_time = (time.time() - start_time) * 1000
                logger.info(f"âœ… Googleåå‘åœ°ç†ç¼–ç ç¼“å­˜å‘½ä¸­ - åæ ‡: ({lat}, {lng}), è€—æ—¶: {elapsed_time:.2f}ms")
                logger.debug(f"ğŸ“ ç¼“å­˜ç»“æœ: name={result.get('name')}, place_id={result.get('place_id')}")
                return result
        
        params = {
            'latlng': f"{lat},{lng}",
            'key': GOOGLE_GEOCODING_CONFIG['api_key'],
            'language': GOOGLE_GEOCODING_CONFIG['language'],
            'region': GOOGLE_GEOCODING_CONFIG['region'],
            'result_type': 'establishment|point_of_interest|premise'
        }
        
        # è®°å½•è¯·æ±‚å‚æ•°ï¼ˆéšè—APIå¯†é’¥ï¼‰
        safe_params = params.copy()
        safe_params['key'] = f"{params['key'][:10]}...{params['key'][-5:]}" if len(params['key']) > 15 else "***"
        logger.info(f"ğŸ” Googleåå‘åœ°ç†ç¼–ç è¯·æ±‚å‚æ•°: {safe_params}")
        
        request_start = time.time()
        response = requests.get(
            GOOGLE_GEOCODING_CONFIG['base_url'],
            params=params,
            timeout=GOOGLE_GEOCODING_CONFIG['timeout']
        )
        request_time = (time.time() - request_start) * 1000
        
        if response.status_code == 200:
            data = response.json()
            
            # è®°å½•APIå“åº”çŠ¶æ€
            logger.info(f"ğŸ“¡ Googleåå‘åœ°ç†ç¼–ç APIå“åº”: status={data.get('status')}, è¯·æ±‚è€—æ—¶: {request_time:.2f}ms")
            
            if data.get('status') == 'OK':
                results = data.get('results', [])
                if results:
                    # æå–å»ºç­‘åç§°
                    building_name = None
                    address_components = results[0].get('address_components', [])
                    for component in address_components:
                        types = component.get('types', [])
                        if 'establishment' in types or 'point_of_interest' in types:
                            building_name = component.get('long_name', '')
                            break
                        elif 'premise' in types:
                            building_name = component.get('long_name', '')
                            break
                    
                    if not building_name:
                        formatted_address = results[0].get('formatted_address', '')
                        if formatted_address and ',' in formatted_address:
                            building_name = formatted_address.split(',')[0].strip()
                        else:
                            building_name = formatted_address
                    
                    if building_name and len(building_name) > 95:
                        building_name = building_name[:95] + "..."
                    
                    if building_name and len(building_name.strip()) >= 3:
                        address_info = {
                            'name': building_name,
                            'place_id': results[0].get('place_id', ''),
                            'types': results[0].get('types', [])
                        }
                        
                        # è®°å½•æˆåŠŸç»“æœ
                        total_time = (time.time() - start_time) * 1000
                        logger.info(f"âœ… Googleåå‘åœ°ç†ç¼–ç æˆåŠŸ - åæ ‡: ({lat}, {lng})")
                        logger.info(f"ğŸ“ ç»“æœ: name={address_info['name']}, place_id={address_info['place_id']}")
                        logger.info(f"â±ï¸  æ€»è€—æ—¶: {total_time:.2f}ms (APIè¯·æ±‚: {request_time:.2f}ms)")
                        
                        # ç¼“å­˜ç»“æœ
                        memory_cache[cache_key] = (time.time(), address_info)
                        return address_info
                    else:
                        logger.warning(f"âš ï¸  Googleåå‘åœ°ç†ç¼–ç ç»“æœæ— æ•ˆ - åæ ‡: ({lat}, {lng}), åç§°: {building_name}")
                else:
                    logger.warning(f"âš ï¸  Googleåå‘åœ°ç†ç¼–ç æ— ç»“æœ - åæ ‡: ({lat}, {lng})")
            else:
                error_msg = f"Googleåå‘åœ°ç†ç¼–ç APIé”™è¯¯: {data.get('status')}"
                if data.get('error_message'):
                    error_msg += f" - {data.get('error_message')}"
                logger.error(f"âŒ {error_msg}")
        else:
            logger.error(f"âŒ Googleåå‘åœ°ç†ç¼–ç HTTPé”™è¯¯: {response.status_code}")
        
        return None
        
    except Exception as e:
        total_time = (time.time() - start_time) * 1000
        logger.error(f"âŒ Googleåå‘åœ°ç†ç¼–ç å¤±è´¥ - åæ ‡: ({lat}, {lng}), é”™è¯¯: {e}, è€—æ—¶: {total_time:.2f}ms")
        return None

def get_google_building_name(lat, lng):
    """è·å–Googleå»ºç­‘åç§°"""
    try:
        logger.debug(f"ğŸ¢ è·å–Googleå»ºç­‘åç§° - åæ ‡: ({lat}, {lng})")
        address_info = google_reverse_geocode(lat, lng)
        if address_info and address_info.get('name'):
            logger.debug(f"ğŸ“ è·å–åˆ°å»ºç­‘åç§°: {address_info['name']}")
            return address_info['name']
        logger.debug(f"âš ï¸  æœªè·å–åˆ°å»ºç­‘åç§° - åæ ‡: ({lat}, {lng})")
        return None
    except Exception as e:
        logger.error(f"âŒ è·å–Googleå»ºç­‘åç§°å¤±è´¥ - åæ ‡: ({lat}, {lng}), é”™è¯¯: {e}")
        return None 